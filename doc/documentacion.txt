Documentación del Proyecto
Objetivo del Proyecto
El desafío consistió en desarrollar un chatbot basado en la arquitectura RAG (Retrieval-Augmented Generation), capaz de responder preguntas sobre el contenido del sitio web y documentos PDF relacionados con la empresa Promtior. Este chatbot utiliza LangChain para la lógica de recuperación y generación de respuestas, y FAISS para el almacenamiento y búsqueda eficiente de embeddings generados.

Implementación
1. Carga de Contenido
El chatbot obtiene la información desde dos fuentes principales:

Contenido del sitio web de Promtior:
Utilizamos requests y BeautifulSoup para extraer el contenido relevante desde el sitio web de Promtior. Esto asegura que el modelo siempre tenga información actualizada.

Documentos PDF:
Para procesar PDFs, empleamos la librería PyPDF2. Esto permite al usuario cargar documentos adicionales desde la interfaz web, ampliando el alcance del chatbot.

2. Vectorización
Los textos extraídos se procesan utilizando embeddings generados con Ollama. Estos embeddings se almacenan en un índice FAISS, que permite búsquedas rápidas y precisas en los documentos.

3. Generación de Respuestas
La arquitectura RAG se configura para recuperar los fragmentos más relevantes desde FAISS. Estos fragmentos se pasan a un modelo generativo (como GPT-3 o Llama), que construye la respuesta final para el usuario.

4. Interfaz Web
El chatbot incluye una interfaz web desarrollada en Flask, donde los usuarios pueden:

Hacer preguntas y recibir respuestas en tiempo real.
Cargar nuevos documentos PDF para enriquecer el conocimiento del modelo.
Ver el historial de la conversación.

Requisitos Previos
Dependencias
Lenguaje y Librerías

Python 3.9 o superior
Flask
LangChain
FAISS
BeautifulSoup
PyPDF2
Requests
Modelo Generativo

Ollama LLM
Debes instalar Ollama en tu máquina local.
Descarga desde Ollama.ai.
Configuración de Ollama
Una vez instalado, asegúrate de que Ollama esté corriendo localmente.
Si es necesario cambiar la configuración del modelo o del puerto, edita el archivo settings.json de Ollama.
Cómo Ejecutar el Proyecto
Clona el Repositorio


git clone https://github.com/JoaquinRamirez98/promtior-rag-chatbot.git
cd promtior-rag-chatbot
Crea un Entorno Virtual


python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
Instala las Dependencias


pip install -r requirements.txt
Ejecuta Ollama Asegúrate de que Ollama esté ejecutándose antes de iniciar el proyecto:


ollama start
Inicia el Chatbot


python app.py
Accede a la Interfaz Web Abre tu navegador y ve a http://127.0.0.1:8000 o sino a https://bf57-181-94-150-70.ngrok-free.app/.

Desafíos Encontrados
1. Problemas con la Instalación de Dependencias
Problema: Incompatibilidades entre versiones de LangChain y FAISS.
Solución: Se utilizó una configuración flexible de dependencias para garantizar la compatibilidad.
2. Restricciones de Ollama en la Nube
Problema: La versión de Ollama local (4 GB) no pudo ser utilizada en Railway debido a limitaciones de tamaño.
Solución: Decidí mantener el procesamiento local para el desarrollo, dado que Railway no soporta este modelo en su configuración actual.
3. Errores en el Despliegue en Railway
Problema: Configuración de variables de entorno y restricciones de memoria.
Solución: Ajusté las configuraciones y validé las dependencias antes del despliegue.
Mejoras Futuras
Integración de Nuevas Fuentes de Datos: Extender el sistema para procesar otros formatos (por ejemplo, Excel o JSON).
Optimización del Despliegue: Implementar una versión optimizada para la nube utilizando un modelo más ligero.
Ampliación de Funcionalidad: Agregar opciones de exportación del historial de conversación.
